---
title: "Part 1"
author: "Jane Huber"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)
library(tidyverse)
library(lubridate)
library(cmfproperty)
library(purrr)
library(gridExtra)
library(ggplot2)

```

```{r download-data, include=FALSE}

con <- DBI::dbConnect(RSQLite::SQLite(), "database/detroit.sqlite")

# convert to tibble
sales <- dplyr::tbl(con, 'sales') %>% dplyr::collect()
assessments <- dplyr::tbl(con, 'assessments') %>% dplyr::collect()
blight <- dplyr::tbl(con, 'blight') %>% dplyr::collect()
parcels <- dplyr::tbl(con, 'parcels') %>% dplyr::collect()
parcels_historic <- dplyr::tbl(con, 'parcels_historic') %>% dplyr::collect()
foreclosures <- dplyr::tbl(con, 'foreclosures') %>% dplyr::collect()

```

### Section A: Exploratory Data Analysis


Conduct an exploratory data analysis of homes in Detroit. Offer an overview of relevant trends in the data and data quality issues. Contextualize your analysis with key literature on properties in Detroit.
https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/
https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf

- What are tools to use for exploratory data analysis? [Chapter 6 of the textbook]

- Step 1: Look into the data, figure out what's going on for each.
Note: Honestly not great descriptions of what different statuses mean. Difficulty in interpreting how I can use these; I'll have to do some poking around in order to make determinations.

- Step 2: clean data so it can be used for analysis later on.
```{r data-exploration-cleaning, include = FALSE}

# Sales
# First, we pick only the columns we care about, then standardize the different types of sales terms (in case we want them some day). Finally, we will filter to only include valid arms length sales, following the methodology of the Center of Municipal Finance at the University of Chicago's study "An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018". Not only does this restrict our data analysis to be more specifically verified, but it will greatly reduce the size of data.
sales_clean <- 
  sales %>% 
  select(c(parcel_num, sale_date, sale_price, sale_terms)) %>% 
  mutate(sale_date = as.numeric(format(as.Date(sale_date), format="%Y")),
         sale_terms = str_replace_all(sale_terms, "Not Arms Length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "not arms length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "valid arms length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "Valid Arms Length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "bank sale used", "BANK SALE USED"),
         sale_terms = str_replace_all(sale_terms, "00-NOT AUDITED", "NOT AUDITED"),
         sale_terms = as_factor(sale_terms)) %>% 
  filter(sale_terms == "VALID ARMS LENGTH")

# Assessments
assessments_clean <-
  assessments %>%
  select(-c(propclass )) %>%
  rename(c(parcel_num = PARCELNO,
           assessed_value = ASSESSEDVALUE,
           taxable_value = TAXABLEVALUE))%>%
  mutate(year = as.factor(year))

# Blight

blight_clean <- 
  blight %>% 
  mutate(violation_date = as.Date(violation_date)) %>% 
  rename(parcel_num = parcelno)

# Parcels: Create a parcels tibble that combines historic with all. Can filter down later if needed.

parcels_current_clean <- 
  parcels %>% 
  rename(parcel_num = parcel_number) %>% 
  mutate(zip_code = as.numeric(zip_code),
         property_class = as.factor(property_class))

parcels_historic_clean <- 
  parcels_historic %>% 
  rename(parcel_num = PARCELNO,
         address = PROPADDR,
         zip_code = ZIPCODE,
         taxpayer_1 = TAXPAYER1,
         taxpayer_street = TAXPADDR,
         taxpayer_city = TAXPCITY,
         taxpayer_state = TAXPSTATE,
         taxpayer_zip = TAXPZIP,
         property_class = propclass,
         tax_status = TAXSTATUS,
         total_square_footage = TOTALSQFT,
         total_acreage = TOTALACREAGE,
         frontage = FRONTAGE,
         homestead_pre = PRE,
         sale_price = SALEPRICE,
         sale_date = SALEDATE,
         assessed_value = ASSESSEDVALUE,
         taxable_value = TAXABLEVALUE) %>% 
  mutate(zip_code = as.numeric(zip_code),
         property_class = as.factor(property_class))

parcels_clean <- 
  left_join(parcels_current_clean, parcels_historic_clean)

# Foreclosures (do I need to do some sort of filtering?)

foreclosures_clean <- 
  foreclosures %>% 
  rename(parcel_num = prop_parcelnum)

sales_and_assessments <- 
  left_join(assessments_clean, sales_clean) %>% 
  filter()

```

The UC Irvine Law Review's "Taxed Out: Illegal Property Tax Assessments and the Epidemic of Tax Foreclosures in Detroit" explores the relationship between over-assessing a home and the ultimate foreclosure of that home. Let's first explore overall assessment values--how do they look over time?--and then see if we can discern any patterns visually in the data between over-assessment and an ultimate foreclosure.


```{r exploratory-analysis, warning = FALSE, message = FALSE}

custom_theme <- theme_bw() +
  theme(plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9), 
        axis.title.y = element_text(size = 9))

# Get distribution of assessment value by year

pacman::p_load(ggridges)

assessments_2011_2016 <- 
  assessments_clean %>% 
  filter(year %in% c(2011, 2012, 2013, 2014, 2015, 2016),
         assessed_value < (sd(assessed_value) * 2)) %>% 
  ggplot() +
  stat_density_ridges(aes(x = assessed_value, y = year),
                      fill = "blue", 
                      colour = "white", 
                      scale = 1, 
                      alpha = 0.6,
                      quantile_lines = TRUE, 
                      quantiles = 2) +
  xlab("Assessment Value") + 
  ylab("Year") + 
  ggtitle("Distribution of Assessements: 2011-2016") +
  custom_theme

assessments_2017_2022 <- 
assessments_clean %>% 
  filter(year %in% c(2017, 2018, 2019, 2020, 2021, 2022),
         assessed_value < (sd(assessed_value) * 2)) %>% 
  ggplot() +
  geom_density_ridges(aes(x = assessed_value, y = year),
                      fill = "blue", 
                      colour = "white", 
                      scale = 1, 
                      alpha = 0.6,
                      quantile_lines = TRUE, 
                      quantiles = 2) +
  xlab("Assessment Value") + 
  ylab("Year") + 
    ggtitle("Distribution of Assessements: 2017-2022") +
  custom_theme

grid.arrange(assessments_2011_2016, assessments_2017_2022, ncol = 2)

# Get distribution of price value by year


```

### Section B: Sales Ratio Study

Use cmfproperty to conduct a sales ratio study across the relevant time period. Note that cmfproperty is designed to produce Rmarkdown reports but use the documentation and insert relevant graphs/figures into your report. Look to make this reproducible since you’ll need these methods to analyze your assessment model later on. Detroit has many sales which are not arm’s length (sold at fair market value) so some sales should be excluded, but which ones?
https://erhla.github.io/cmfproperty/articles/cmfproperty.html

Things I want to write:
- What sort of data processing am I doing? What am I removing?
- Lean heavily on the strategy of the CCAO to exclude, then do that. Include
- a link.

Remove data:
- Drop certain sales because of sale_terms; should look at what they mean
- Drop certain sales if they are X (5?) number of standard deviations from the assessment, just like how the CCAO did it. In order to do this, I want to first standardize all of them... There are some goofy different ways of framing this.




```{r cmfproperty-sales-ratio, warning = FALSE, message = FALSE}
# 
# ratios <-
#   cmfproperty::reformat_data(
#     data = sales_and_assessments,
#     sale_col = "sale_price",
#     assessment_col = "assessed_value",
#     sale_year_col = "sale_date",
#   )
# 
# head(as.data.frame(ratios))
# 
# 
# 
# stats <- cmfproperty::calc_iaao_stats(ratios)
# head(stats)
# 
# output <- diagnostic_plots(stats, 
#                            ratios, 
#                            min_reporting_yr = 2015, 
#                            max_reporting_yr = 2022)
# 
# output[[1]]
# output[[2]]
# output[[3]]
# output[[4]]

# cmfproperty::make_report(ratios,
#                          jurisdiction_name = "Detroit",
#                          output_dir = "~/Documents/")

```


## Section C: Explore trends and relationships with property sales using simple regressions

```{r property-sales-regressions}


```

## Section D: Explore trends and relationships with foreclosures using simple regressions

```{r foreclosures-regressions}


```