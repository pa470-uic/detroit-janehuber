---
title: "Part 1"
author: "Jane Huber"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)
library(tidyverse)
library(lubridate)
library(cmfproperty)
library(purrr)
library(gridExtra)
library(ggplot2)
library(readxl)

```

```{r download-data, include=FALSE}

con <- DBI::dbConnect(RSQLite::SQLite(), "../database/detroit.sqlite")

# convert to tibble
sales <- dplyr::tbl(con, 'sales') %>% dplyr::collect()
assessments <- dplyr::tbl(con, 'assessments') %>% dplyr::collect()
foreclosures <- dplyr::tbl(con, 'foreclosures') %>% dplyr::collect()

property_classifications <- read_xlsx("../files/OFFICE OF THE ASSESSORS_PROPERTY CLASSIFICATIONS -rev.xlsx")

```

### Section A: Exploratory Data Analysis


Conduct an exploratory data analysis of homes in Detroit. Offer an overview of relevant trends in the data and data quality issues. Contextualize your analysis with key literature on properties in Detroit.
https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/
https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf


```{r data-exploration-cleaning, include = FALSE}

# Sales
# First, we pick only the columns we care about, then standardize the different types of sales terms (in case we want them some day). Finally, we will filter to only include valid arms length sales, following the methodology of the Center of Municipal Finance at the University of Chicago's study "An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018". Not only does this restrict our data analysis to be more specifically verified, but it will greatly reduce the size of data.
sales_clean <- 
  sales %>% 
  select(c(parcel_num, sale_date, sale_price, sale_terms)) %>% 
  mutate(sale_date = as_factor(format(as.Date(sale_date), format="%Y")),
         sale_terms = str_replace_all(sale_terms, "Not Arms Length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "not arms length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "valid arms length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "Valid Arms Length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "bank sale used", "BANK SALE USED"),
         sale_terms = str_replace_all(sale_terms, "00-NOT AUDITED", "NOT AUDITED"),
         sale_terms = as_factor(sale_terms)) %>% 
  filter(sale_terms == "VALID ARMS LENGTH") 

# Assessments
assessments_clean <-
  assessments %>%
  select(-c(propclass)) %>%
  rename(c(parcel_num = PARCELNO,
           assessed_value = ASSESSEDVALUE,
           taxable_value = TAXABLEVALUE,
           assessment_year = year))

assessments_clean_wider <-
  assessments_clean %>% 
  pivot_wider(names_from = assessment_year,
              values_from = assessed_value)

years_for_evaluation <- c('2016', '2017', '2018', '2019')


foreclosures_clean <- 
  foreclosures %>% 
  rename(parcel_num = prop_parcelnum) %>% 
  pivot_longer(cols = years_for_evaluation,
               names_to = "year_foreclosed",
               values_to = "status",
               values_drop_na = TRUE
               ) %>% 
  filter(year_foreclosed >= 2016,
         year_foreclosed <= 2019) %>% 
  select(c(parcel_num, year_foreclosed))

sales_and_assessments <- 
  left_join(sales_clean, assessments_clean) %>% 
  filter(!is.na(sale_date),
         sale_date > 2016,
         assessment_year > 2011)

sales_assessments_foreclosures <- 
  left_join(sales_and_assessments, foreclosures_clean)

```

The UC Irvine Law Review's ["Taxed Out: Illegal Property Tax Assessments and the Epidemic of Tax Foreclosures in Detroit"](https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/) explores the relationship between over-assessing a home and the ultimate foreclosure of that home. Let's first explore overall assessment values--how do they look over time?--and then see if we can discern any patterns visually in the data between over-assessment and an ultimate foreclosure.

```{r exploratory-analysis-assessments, message=FALSE}

custom_theme <- theme_bw() +
  theme(plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9), 
        axis.title.y = element_text(size = 9))


# Get distribution of assessment value by year

pacman::p_load(ggridges)

# assessments_2011_2016 <- 
#   assessments_clean %>% 
#   filter(assessment_year %in% c(2011, 2012, 2013, 2014, 2015, 2016),
#          assessed_value < (sd(assessed_value) * 2),
#          assessed_value > 0) %>% 
#   mutate(assessment_year = as.factor(assessment_year)) %>% 
#   ggplot() +
#   stat_density_ridges(aes(x = assessed_value, y = assessment_year),
#                       fill = "blue", 
#                       colour = "white", 
#                       scale = 1, 
#                       alpha = 0.6,
#                       quantile_lines = TRUE, 
#                       quantiles = 2) +
#   xlab("Assessment Value") + 
#   ylab("Year") + 
#   ggtitle("Distribution of Assessements: 2011-2016") +
#   custom_theme

assessments_2016_2022 <- 
assessments_clean %>% 
  filter(assessment_year %in% c(2016, 2017, 2018, 2019, 2020, 2021, 2022),
         assessed_value < (sd(assessed_value) * 2),
         assessed_value > 0) %>% 
  mutate(assessment_year = as.factor(assessment_year)) %>% 
  ggplot() +
  geom_density_ridges(aes(x = assessed_value, y = assessment_year),
                      fill = "blue", 
                      colour = "white", 
                      scale = 1, 
                      alpha = 0.6,
                      quantile_lines = TRUE, 
                      quantiles = 2) +
  xlab("Assessment Value") + 
  ylab("Year") + 
    ggtitle("Distribution of Assessements: 2017-2022") +
  custom_theme

assessments_2016_2022

#grid.arrange(assessments_2011_2016, assessments_2017_2022, ncol = 2)

```

Here, we can see that the average assessment decreased over time. Now, let's look at sales over time alongside those assessments; are home values also going down?

```{r exploratory-analysis-sales, message=FALSE}

sales_trends_over_time <- 
  sales_clean %>% 
  filter(sale_price < (sd(sale_price) * 3)) %>% 
  ggplot() +
  geom_boxplot(aes(sale_date,sale_price)) +
  xlab("Year") + 
  ylab("Price") + 
  ggtitle("Detroit Home Sales") +
  custom_theme

sales_trends_over_time

```


The Harris School of Public Policy at the University of Chicago's The Center For Municipal Finance produced the report ["An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018"](https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf) emphasizes the reduction of the number of assessments higher than sales, but that it's still too high. Here, we can reproduce that finding with the data:

```{r percentage-assessment-higher-sale, message = FALSE} 
sales_and_assessments_data <- 
  sales_and_assessments %>% 
  select(-c(sale_terms, taxable_value)) %>% 
  filter(!is.na(sale_price)) %>% 
  mutate(assessment_sale_diff = (assessed_value - sale_price)) %>% 
  select(c(sale_date, assessment_sale_diff)) %>% 
  group_by(sale_date) %>% 
  summarize(perc_assessment_higher = (sum(assessment_sale_diff >= 0)/n()))

sales_and_assessment_difference_time <- 
  sales_and_assessments_data %>% 
  ggplot(aes(x = sale_date, y = perc_assessment_higher)) +
  geom_line() +
  xlab("Year") + 
  ylab("Percent") + 
  ggtitle("Percentage of Assessments Higher Than Sales in Detroit") +
  custom_theme

sales_and_assessment_difference_time

```


### Section B: Sales Ratio Study

Use cmfproperty to conduct a sales ratio study across the relevant time period. Note that cmfproperty is designed to produce Rmarkdown reports but use the documentation and insert relevant graphs/figures into your report. Look to make this reproducible since you’ll need these methods to analyze your assessment model later on. Detroit has many sales which are not arm’s length (sold at fair market value) so some sales should be excluded, but which ones?
https://erhla.github.io/cmfproperty/articles/cmfproperty.html

Things I want to write:
- What sort of data processing am I doing? What am I removing?
- Lean heavily on the strategy of the CCAO to exclude, then do that. Include
- a link.

Remove data:
- Drop certain sales because of sale_terms; should look at what they mean
- Drop certain sales if they are X (5?) number of standard deviations from the assessment, just like how the CCAO did it. In order to do this, I want to first standardize all of them... There are some goofy different ways of framing this.




```{r cmfproperty-sales-ratio, warning = FALSE, message = FALSE}


ratios <-
  cmfproperty::reformat_data(
    data = sales_and_assessments,
    sale_col = "sale_price",
    assessment_col = "assessed_value",
    sale_year_col = "sale_date",
  )

head(as.data.frame(ratios))



stats <- cmfproperty::calc_iaao_stats(ratios)
head(stats)

output <- diagnostic_plots(stats,
                           ratios,
                           min_reporting_yr = 2016,
                           max_reporting_yr = 2019)

output[[1]]
output[[2]]
output[[3]]
output[[4]]

# cmfproperty::make_report(ratios,
#                          jurisdiction_name = "Detroit",
#                          output_dir = "~/Documents/")

```


### Section C: Explore trends and relationships with property sales using simple regressions

First goal: use linear regression

Second goal: use yardstick (and broom??) to evaluate. Move on.


```{r property-sales-regressions}









```

### Section D: Explore trends and relationships with foreclosures using simple regressions

```{r foreclosures-regressions}


```