---
title: "Part 1"
author: "Jane Huber"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE)
library(tidyverse)
library(lubridate)
library(cmfproperty)
library(purrr)
library(gridExtra)
library(ggplot2)
library(readxl)
library(tidymodels)

```

```{r download-data, include=FALSE}

con <- DBI::dbConnect(RSQLite::SQLite(), "../database/detroit.sqlite")

# convert to tibble
sales <- dplyr::tbl(con, 'sales') %>% dplyr::collect()
assessments <- dplyr::tbl(con, 'assessments') %>% dplyr::collect()
foreclosures <- dplyr::tbl(con, 'foreclosures') %>% dplyr::collect()

property_classifications <- read_xlsx("../files/OFFICE OF THE ASSESSORS_PROPERTY CLASSIFICATIONS -rev.xlsx")

```

### Section A: Exploratory Data Analysis


Conduct an exploratory data analysis of homes in Detroit. Offer an overview of relevant trends in the data and data quality issues. Contextualize your analysis with key literature on properties in Detroit.
https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/
https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf


```{r data-exploration-cleaning, include = FALSE}
# Sales
# First, we pick only the columns we care about, then standardize the different types of sales terms (in case we want them some day). Finally, we will filter to only include valid arms length sales, following the methodology of the Center of Municipal Finance at the University of Chicago's study "An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018". Not only does this restrict our data analysis to be more specifically verified, but it will greatly reduce the size of data.

years_for_evaluation <- c('2016', '2017', '2018', '2019', '2020')

#remove factor for sale date
sales_clean <- 
  sales %>% 
  select(c(parcel_num, sale_date, sale_price, sale_terms, property_c)) %>% 
  rename(c(prop_class = property_c)) %>% 
  mutate(sale_date = format(as.Date(sale_date), format="%Y"),
         sale_terms = str_replace_all(sale_terms, "Not Arms Length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "not arms length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "valid arms length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "Valid Arms Length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "bank sale used", "BANK SALE USED"),
         sale_terms = str_replace_all(sale_terms, "00-NOT AUDITED", "NOT AUDITED"),
         prop_class = as_factor(prop_class), 
         sale_terms = as_factor(sale_terms)) %>% 
  filter(sale_terms == "VALID ARMS LENGTH",
         sale_date %in% years_for_evaluation)

# Assessments
assessments_clean <-
  assessments %>%
  rename(c(parcel_num = PARCELNO,
           assessed_value = ASSESSEDVALUE,
           taxable_value = TAXABLEVALUE,
           assessment_year = year,
           prop_class = propclass))

sales_and_assessments <- 
  left_join(sales_clean, assessments_clean) %>% 
  filter(!is.na(sale_date)) %>% 
  mutate(sale_date = as.numeric(sale_date))

```

The UC Irvine Law Review's ["Taxed Out: Illegal Property Tax Assessments and the Epidemic of Tax Foreclosures in Detroit"](https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/) explores the relationship between over-assessing a home and the ultimate foreclosure of that home. Let's first explore overall assessment values--how do they look over time?--and then see if we can discern any patterns visually in the data between over-assessment and an ultimate foreclosure.

```{r exploratory-analysis-assessments}

custom_theme <- theme_bw() +
  theme(plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9), 
        axis.title.y = element_text(size = 9))


# Get distribution of assessment value by year

pacman::p_load(ggridges)

assessments_2016_2018 <-
  assessments_clean %>%
  filter(assessment_year %in% c(2016, 2017, 2018),
         assessed_value < (sd(assessed_value) * 2),
         assessed_value > 0) %>%
  mutate(assessment_year = as.factor(assessment_year)) %>%
  ggplot() +
  stat_density_ridges(aes(x = assessed_value, y = assessment_year),
                      fill = "blue",
                      colour = "white",
                      scale = 1,
                      alpha = 0.6,
                      quantile_lines = TRUE,
                      quantiles = 2) +
  xlab("Assessment Value") +
  ylab("Year") +
  ggtitle("Distribution of Assessements: 2016-2018") +
  custom_theme

assessments_2019_2021 <- 
assessments_clean %>% 
  filter(assessment_year %in% c(2019, 2020, 2021),
         assessed_value < (sd(assessed_value) * 2),
         assessed_value > 0) %>% 
  mutate(assessment_year = as.factor(assessment_year)) %>% 
  ggplot() +
  geom_density_ridges(aes(x = assessed_value, y = assessment_year),
                      fill = "blue", 
                      colour = "white", 
                      scale = 1, 
                      alpha = 0.6,
                      quantile_lines = TRUE, 
                      quantiles = 2) +
  xlab("Assessment Value") + 
  ylab("Year") + 
    ggtitle("Distribution of Assessements: 2019-2021") +
  custom_theme

grid.arrange(assessments_2016_2018, assessments_2019_2021, ncol = 2)

```

Here, we can see that the average assessment decreased over time. Now, let's look at sales over time alongside those assessments; are home values also going down?

```{r exploratory-analysis-sales}

sales_trends_over_time <- 
  sales_clean %>% 
  filter(sale_price < (sd(sale_price) * 3)) %>% 
  ggplot() +
  geom_boxplot(aes(sale_date,sale_price)) +
  xlab("Year") + 
  ylab("Price") + 
  ggtitle("Detroit Home Sales") +
  custom_theme

sales_trends_over_time

```


The Harris School of Public Policy at the University of Chicago's The Center For Municipal Finance produced the report ["An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018"](https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf) emphasizes the reduction of the number of assessments higher than sales, but that it's still too high. Here, we can reproduce that finding with the data:


We can also see that assessments 
```{r percentage-assessment-higher-sale} 
sales_and_assessment_differences <- 
  sales_and_assessments %>% 
  select(-c(sale_terms, taxable_value)) %>% 
  filter(!is.na(sale_price),
         sale_price <  1000000) %>% 
  mutate(assessment_sale_diff = (assessed_value - sale_price)) %>% 
  filter(assessment_sale_diff < (sd(assessment_sale_diff) * 2))

sales_and_assessment_differences_graph <- 
  sales_and_assessment_differences %>% 
  ggplot(aes(x = sale_price, y = assessment_sale_diff)) +
  geom_point(size = 0.5, alpha = 0.2, colour="navy")+
  xlab("Year") + 
  ylab("Difference") + 
  ggtitle("Difference in Dollars Between of Assessments And Sales in Detroit") +
  custom_theme

sales_and_assessment_differences_graph

```


### Section B: Sales Ratio Study

Use cmfproperty to conduct a sales ratio study across the relevant time period. Note that cmfproperty is designed to produce Rmarkdown reports but use the documentation and insert relevant graphs/figures into your report. Look to make this reproducible since you’ll need these methods to analyze your assessment model later on. Detroit has many sales which are not arm’s length (sold at fair market value) so some sales should be excluded, but which ones?
https://erhla.github.io/cmfproperty/articles/cmfproperty.html

Things I want to write:
- What sort of data processing am I doing? What am I removing?
- Lean heavily on the strategy of the CCAO to exclude, then do that. Include
- a link.


```{r cmfproperty-sales-ratio}

ratios <-
  cmfproperty::reformat_data(
    data = sales_and_assessments,
    sale_col = "sale_price",
    assessment_col = "assessed_value",
    sale_year_col = "sale_date",
  )

# head(as.data.frame(ratios))
#

stats <- cmfproperty::calc_iaao_stats(ratios)
# head(stats)

output <- diagnostic_plots(stats,
                           ratios,
                           min_reporting_yr = 2016,
                           max_reporting_yr = 2019)

# Number of Arm's Length Sales
# output[[1]]

```


```{r cmfproperty-sale-price-assessed-value}
# Sale Price (solid) & Assessed Value (Dashed)
output[[2]]
```

```{r cmfproperty-ratio-sale-price-assessed-value}
#Ratio between Assessed Value and Sale Price
output[[3]]
```

```{r cmfproperty-inflation-sale-assessed}
# # Inflation Adjusted Sale Price and Assessed Value
output[[4]]
```

### Section C: Explore trends and relationships with property sales using simple regressions

First goal: use linear regression

Second goal: use yardstick (and broom??) to evaluate. Move on.


```{r property-sales-regressions}

 lm(sale_price ~ prop_class + sale_date, data = sales_clean)



lm_sales <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression") %>%
  fit(sale_price ~ prop_class + sale_date,
      data = sales_clean)

lm_sales %>% tidy()


```

### Section D: Explore trends and relationships with foreclosures using simple regressions

```{r foreclosures-regressions}


```