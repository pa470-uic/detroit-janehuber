---
title: "Part 2"
author: "Jane Huber"
date: "2/20/2022"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE)
library(tidyverse)
library(lubridate)
library(cmfproperty)
library(purrr)
library(gridExtra)
library(ggplot2)
library(readxl)
library(tidymodels)
library(glmnet)
```

### Part A

Homes in Detroit have suffered from poorly assessed home values, which has contributed greatly to foreclosure during a time of economic instability. The UC Irvine Law Review's ["Taxed Out: Illegal Property Tax Assessments and the Epidemic of Tax Foreclosures in Detroit"](https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/) analyzes  the relationship between over-assessing a home and its role in driving foreclosures, exposing government's role in creating conditions for people to lose their homes. The Harris School of Public Policy at the University of Chicago's The Center For Municipal Finance produced the report ["An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018"](https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf) shows how the city of Detroit attempted to correct that mistake, reducing the number of assessments higher than sales, but that it's still too high.


```{r download-data, include=FALSE}

con <- DBI::dbConnect(RSQLite::SQLite(), "../database/detroit.sqlite")

# convert to tibble
sales <- dplyr::tbl(con, 'sales') %>% dplyr::collect()
assessments <- dplyr::tbl(con, 'assessments') %>% dplyr::collect()
foreclosures <- dplyr::tbl(con, 'foreclosures') %>% dplyr::collect()
parcels <- dplyr::tbl(con, 'parcels') %>% dplyr::collect()
parcels_historic <- dplyr::tbl(con, 'parcels_historic') %>% dplyr::collect()

custom_theme <- theme_bw() +
  theme(plot.title = element_text(size = 9),
        axis.title.x = element_text(size = 9), 
        axis.title.y = element_text(size = 9))

# property_classifications <- read_xlsx("files/OFFICE OF THE ASSESSORS_PROPERTY CLASSIFICATIONS -rev.xlsx")
```

```{r data-exploration-cleaning, include = FALSE}
# Sales
# First, we pick only the columns we care about, then standardize the different types of sales terms (in case we want them some day). Finally, we will filter to only include valid arms length sales, following the methodology of the Center of Municipal Finance at the University of Chicago's study "An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018". Not only does this restrict our data analysis to be more specifically verified, but it will greatly reduce the size of data.

years_for_evaluation <- c('2016', '2017', '2018', '2019')

#remove factor for sale date
sales_clean <- 
  sales %>% 
  select(c(parcel_num, sale_date, sale_price, sale_terms, property_c)) %>% 
  rename(c(prop_class = property_c)) %>% 
  mutate(sale_date = format(as.Date(sale_date), format="%Y"),
         sale_terms = str_replace_all(sale_terms, "Not Arms Length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "not arms length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "valid arms length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "Valid Arms Length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "bank sale used", "BANK SALE USED"),
         sale_terms = str_replace_all(sale_terms, "00-NOT AUDITED", "NOT AUDITED"),
         prop_class = as_factor(prop_class), 
         sale_terms = as_factor(sale_terms)) %>% 
  filter(sale_terms == "VALID ARMS LENGTH",
         sale_date %in% years_for_evaluation,
         prop_class == '401')

# Assessments
assessments_clean <-
  assessments %>%
  rename(c(parcel_num = PARCELNO,
           assessed_value = ASSESSEDVALUE,
           taxable_value = TAXABLEVALUE,
           assessment_year = year,
           prop_class = propclass)) %>% 
  mutate(prop_class = as_factor(prop_class)) %>% 
  filter(assessed_value > 2000)

# Parcels: Create a parcels tibble that combines historic with all. Can filter down later if needed.
parcels_current_clean <-
  parcels %>%
  rename(parcel_num = parcel_number) %>%
  mutate(zip_code = as.numeric(zip_code),
         property_class = as.factor(property_class))

parcels_historic_clean <-
  parcels_historic %>%
  rename(parcel_num = PARCELNO,
         address = PROPADDR,
         zip_code = ZIPCODE,
         taxpayer_1 = TAXPAYER1,
         taxpayer_street = TAXPADDR,
         taxpayer_city = TAXPCITY,
         taxpayer_state = TAXPSTATE,
         taxpayer_zip = TAXPZIP,
         property_class = propclass,
         tax_status = TAXSTATUS,
         total_square_footage = TOTALSQFT,
         total_acreage = TOTALACREAGE,
         frontage = FRONTAGE,
         homestead_pre = PRE,
         sale_price = SALEPRICE,
         sale_date = SALEDATE,
         assessed_value = ASSESSEDVALUE,
         taxable_value = TAXABLEVALUE) %>%
  mutate(zip_code = as.numeric(zip_code),
         property_class = as.factor(property_class)) %>% 
  filter(!is.na(total_square_footage))

parcels_clean <-
  left_join(parcels_current_clean, parcels_historic_clean) %>% 
  select(c(ward, parcel_num, council_district, zip_code, total_square_footage)) %>% 
  mutate(zip_code = as.numeric(zip_code))


sales_and_assessments <- 
  left_join(sales_clean, assessments_clean) %>% 
  filter(!is.na(sale_date)) %>% 
  mutate(sale_date = as.numeric(sale_date))

sales_assessments_parcels <- 
  left_join(sales_and_assessments, parcels_clean) 
```

Big picture, the distribution of assessment values for Detroit has widened, with the mean assessment increasing over time. However, we can still see that there is a concentration of assessment values for lower home valuation. 

```{r exploratory-analysis-assessments}


# Get distribution of assessment value by year

pacman::p_load(ggridges)

assessments_2016_2018 <-
  assessments_clean %>%
  filter(assessment_year %in% c(2016, 2017, 2018),
         assessed_value < (sd(assessed_value) * 2),
         assessed_value > 0) %>%
  mutate(assessment_year = as.factor(assessment_year)) %>%
  ggplot() +
  stat_density_ridges(aes(x = assessed_value, y = assessment_year),
                      fill = "blue",
                      colour = "white",
                      scale = 1,
                      alpha = 0.6,
                      quantile_lines = TRUE,
                      quantiles = 2) +
  xlab("Assessment Value") +
  ylab("Year") +
  ggtitle("Distribution of Assessements: 2016-2018") +
  custom_theme

assessments_2019_2021 <- 
assessments_clean %>% 
  filter(assessment_year %in% c(2019, 2020, 2021),
         assessed_value < (sd(assessed_value) * 2),
         assessed_value > 0) %>% 
  mutate(assessment_year = as.factor(assessment_year)) %>% 
  ggplot() +
  geom_density_ridges(aes(x = assessed_value, y = assessment_year),
                      fill = "blue", 
                      colour = "white", 
                      scale = 1, 
                      alpha = 0.6,
                      quantile_lines = TRUE, 
                      quantiles = 2) +
  xlab("Assessment Value") + 
  ylab("Year") + 
    ggtitle("Distribution of Assessements: 2019-2021") +
  custom_theme

grid.arrange(assessments_2016_2018, assessments_2019_2021, ncol = 2)

```

Even as assessments decrease over time, overall sale values have been increasing. This tells us that assessment values are attempting to better respond to and assess more expensive homes for sale in Detroit.

```{r exploratory-analysis-sales, message=FALSE}

sales_trends_over_time <- 
  sales_clean %>% 
  filter(sale_price < (sd(sale_price) * 3)) %>% 
  ggplot() +
  geom_boxplot(aes(sale_date,sale_price)) +
  xlab("Year") + 
  ylab("Price") + 
  ggtitle("Detroit Home Sales") +
  custom_theme

sales_trends_over_time

```


```{r cmfproperty-sales-ratio}

ratios <-
  cmfproperty::reformat_data(
    data = sales_and_assessments,
    sale_col = "sale_price",
    assessment_col = "assessed_value",
    sale_year_col = "sale_date",
  )

stats <- cmfproperty::calc_iaao_stats(ratios)

output <- diagnostic_plots(stats,
                           ratios,
                           min_reporting_yr = 2016,
                           max_reporting_yr = 2019)


```

Detroit's assessment values across percentile home values appear to have remained steady in dollars since 2016, even as home assessments increased over time. It is important to note, however, that the red line (homes at the 25th percentile of sale price) have the smallest gap between their assessed value and their sale price. This helps us to see the way in which inequality has been exacerbated by assessing homes at a much closer value to their sale price than for more expensive homes.

```{r cmfproperty-sale-price-assessed-value}
# Sale Price (solid) & Assessed Value (Dashed)
output[[2]]
```


Detroit aims to assess homes at 50% their value, however they have historically overassessed at a relatively alarming rate. We can see how they have attempted to correct for this overassessment since 2016, but nevertheless, a large number of homes remain overassessed.

```{r cmfproperty-ratio-sale-price-assessed-value}
#Ratio between Assessed Value and Sale Price
output[[3]]
```



### Part B

Let's construct a model to determine whether or not a home was overassessed in 2016. We will use logistic regression to help us predict whether something will be "overassessed" or "underassessed". In Detroit, homes are assessed at 50% value--meaning that if a home was sold for \$100,000, it would be assessed for $50,000.  

```{r classify-home-as-overassessed, include=TRUE}

# Get data filtered
overassessed <- 
sales_assessments_parcels %>% 
  filter(sale_date == 2016,
         assessment_year == 2016) %>% 
  mutate(overassessed = as_factor(if_else(sale_price < (assessed_value * 2),
                                "YES",
                                "NO")))

split <- rsample::initial_split(overassessed)

train <- training(split)
test <- testing(split)


# Wanted to use a random forest here, but couldn't figure out the best way to visualize.

# Additionally, I tried to include total_square_footage in the model, and was unable to get any "YES" in my graphs... 
# Would love to discuss in class further.

# rand_forest_model <- 
# rand_forest(trees = 1000, min_n = 5) %>% 
#   set_engine("ranger", verbose = TRUE) %>% 
#   set_mode("classification") %>%
#   translate()

log_model <- 
  logistic_reg() %>%
  set_engine('glm') %>%
  set_mode('classification')

overassessed_recipe <- 
  recipe(overassessed ~ sale_price + ward + zip_code,
      data = train) %>%
  step_other(ward, threshold = 0.01) %>% 
  step_other(zip_code, threshold = 0.01) %>% 
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())
  
wflow <- 
  workflow() %>%
  add_model(log_model) %>% 
  add_recipe(overassessed_recipe)


#Evaluate model
rf_fit <- fit(wflow, overassessed)

homes_2016 <- 
  train %>% 
  filter(sale_date == 2016)

overassessment_predictions <- 
  augment(rf_fit, homes_2016)

```

After running our model, it is relatively effective at determining whether or not a property will be overassesed.

```{r evaluate-overassessments}

overassment_evaluation <- 
  left_join(overassessment_predictions, overassessed) %>% 
  group_by(overassessed) %>% 
  summarize(predicted_not_overassessed = sum(.pred_class == "NO"),
            predicted_overassessed = sum(.pred_class == "YES"))

overassment_evaluation


# Create a classifier metrics table to help us further evaluate our model.

true_negative <- overassment_evaluation$predicted_not_overassessed[1]
false_negative <- overassment_evaluation$predicted_not_overassessed[2]
false_positive <- overassment_evaluation$predicted_overassessed[1]
true_positive <- overassment_evaluation$predicted_overassessed[2]

true_positive_rate <- true_positive / (true_positive + false_negative)
true_negative_rate <- true_negative / (true_negative + false_positive)
false_positive_rate <- false_positive /(false_positive + true_negative)
false_negative_rate <- false_negative / (false_negative + true_positive)
positive_predictive_value <- true_positive / (true_positive + false_positive)

classifier_metrics_table <- tibble(measurement = c("true positive rate", 
                                                   "true negative rate",
                                                   "false positive rate",
                                                   "false positive rate",
                                                   "positive predictive value"), 
                                   `value` = c(true_positive_rate, 
                                               true_negative_rate,
                                               false_positive_rate,
                                               false_negative_rate,
                                               positive_predictive_value))

classifier_metrics_table

```

Furthermore, an ROC curve further demonstrates the effectiveness of our model is at determining whether a property is likely to be overassessed.

```{r roc-curve-overassessments}

roc_curve <- yardstick::roc_curve(data = overassessment_predictions, truth = overassessed, .pred_NO)

autoplot(roc_curve)
```

### Part C

Next, we will create a model to predict sale prices for homes in 2019.

```{r sale-predictions-2019}

split_sales <- rsample::initial_split(sales_assessments_parcels)

train_sales <- training(split_sales)
test_sales <- testing(split_sales)

linear_model_sales <- 
  linear_reg() %>% 
  set_engine("lm") 

sale_price_recipe <- 
  recipe(sale_price ~ ward + zip_code + assessed_value,
      data = train_sales) %>%
  step_other(ward, threshold = 0.01) %>% 
  step_other(zip_code, threshold = 0.01) %>% 
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())
  
sale_wflow <- 
  workflow() %>%
  add_model(linear_model_sales) %>% 
  add_recipe(sale_price_recipe)

# Fit the model
rf_fit_sales <- fit(sale_wflow, train_sales)

rf_fit_sales %>% tidy() %>% view()

sales_2019 <- 
  test_sales %>% 
  filter(sale_date == 2019)

sale_price_predictions <- 
  augment(rf_fit_sales, sales_2019)

```

Now that we have run the model, we will use MAPE as a first evaluation.
```{r mape-analysis}

#I don't understand why I'm getting an Infinite value here...
yardstick::mape(sale_price_predictions,
     truth = sale_price,
     estimate = .pred)
```

Next, we will use RMSE to further evaluate the model.

```{r rmse-analysis}
yardstick::rmse(sale_price_predictions,
     truth = sale_price,
     estimate = .pred)

```
