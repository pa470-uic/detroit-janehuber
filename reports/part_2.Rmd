---
title: "Part 2"
author: "Jane Huber"
date: "2/20/2022"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    theme: sandstone
    toc: true
editor_options: 
  chunk_output_type: console
---
Questions to ask in class:

1. Can we go over classification tables 8.3 and 8.4 from the textbook and the classification probability metric ROC curves? Specifically, the metric ROC curves.
2. 



```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE)
library(tidyverse)
library(lubridate)
library(cmfproperty)
library(purrr)
library(gridExtra)
library(ggplot2)
library(readxl)
library(tidymodels)
```

```{r download-data, include=FALSE}

con <- DBI::dbConnect(RSQLite::SQLite(), "database/detroit.sqlite")

# convert to tibble
sales <- dplyr::tbl(con, 'sales') %>% dplyr::collect()
assessments <- dplyr::tbl(con, 'assessments') %>% dplyr::collect()
foreclosures <- dplyr::tbl(con, 'foreclosures') %>% dplyr::collect()
parcels <- dplyr::tbl(con, 'parcels') %>% dplyr::collect()
parcels_historic <- dplyr::tbl(con, 'parcels_historic') %>% dplyr::collect()

# property_classifications <- read_xlsx("files/OFFICE OF THE ASSESSORS_PROPERTY CLASSIFICATIONS -rev.xlsx")
```

```{r data-exploration-cleaning, include = FALSE}
# Sales
# First, we pick only the columns we care about, then standardize the different types of sales terms (in case we want them some day). Finally, we will filter to only include valid arms length sales, following the methodology of the Center of Municipal Finance at the University of Chicago's study "An Evaluation of Residential Property Tax Assessments in the City of Detroit, 2016-2018". Not only does this restrict our data analysis to be more specifically verified, but it will greatly reduce the size of data.

years_for_evaluation <- c('2016', '2017', '2018', '2019')

#remove factor for sale date
sales_clean <- 
  sales %>% 
  select(c(parcel_num, sale_date, sale_price, sale_terms, property_c)) %>% 
  rename(c(prop_class = property_c)) %>% 
  mutate(sale_date = format(as.Date(sale_date), format="%Y"),
         sale_terms = str_replace_all(sale_terms, "Not Arms Length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "not arms length", "NOT ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "valid arms length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "Valid Arms Length", "VALID ARMS LENGTH"),
         sale_terms = str_replace_all(sale_terms, "bank sale used", "BANK SALE USED"),
         sale_terms = str_replace_all(sale_terms, "00-NOT AUDITED", "NOT AUDITED"),
         prop_class = as_factor(prop_class), 
         sale_terms = as_factor(sale_terms)) %>% 
  filter(sale_terms == "VALID ARMS LENGTH",
         sale_date %in% years_for_evaluation,
         prop_class == '401')

# Assessments
assessments_clean <-
  assessments %>%
  rename(c(parcel_num = PARCELNO,
           assessed_value = ASSESSEDVALUE,
           taxable_value = TAXABLEVALUE,
           assessment_year = year,
           prop_class = propclass)) %>% 
  mutate(prop_class = as_factor(prop_class)) %>% 
  filter(assessed_value > 2000)

# Parcels: Create a parcels tibble that combines historic with all. Can filter down later if needed.
parcels_current_clean <-
  parcels %>%
  rename(parcel_num = parcel_number) %>%
  mutate(zip_code = as.numeric(zip_code),
         property_class = as.factor(property_class))

parcels_historic_clean <-
  parcels_historic %>%
  rename(parcel_num = PARCELNO,
         address = PROPADDR,
         zip_code = ZIPCODE,
         taxpayer_1 = TAXPAYER1,
         taxpayer_street = TAXPADDR,
         taxpayer_city = TAXPCITY,
         taxpayer_state = TAXPSTATE,
         taxpayer_zip = TAXPZIP,
         property_class = propclass,
         tax_status = TAXSTATUS,
         total_square_footage = TOTALSQFT,
         total_acreage = TOTALACREAGE,
         frontage = FRONTAGE,
         homestead_pre = PRE,
         sale_price = SALEPRICE,
         sale_date = SALEDATE,
         assessed_value = ASSESSEDVALUE,
         taxable_value = TAXABLEVALUE) %>%
  mutate(zip_code = as.numeric(zip_code),
         property_class = as.factor(property_class))

parcels_clean <-
  left_join(parcels_current_clean, parcels_historic_clean) %>% 
  select(c(ward, parcel_num, council_district, zip_code))


sales_and_assessments <- 
  left_join(sales_clean, assessments_clean) %>% 
  filter(!is.na(sale_date)) %>% 
  mutate(sale_date = as.numeric(sale_date))

sales_assessments_parcels <- 
  left_join(sales_and_assessments, parcels_clean) 
```
Decision here: do I want to use logistic regression or random forest to attempt to classify predictions?

Remember: use data from 2016.

Evaluate using: classification metrics from tables 8.3 and 8.4 from the textbook and the classification probability metric ROC curves.

```{r classify-home-as-overassessed}

# Get data filtered
overassessed <- 
sales_assessments_parcels %>% 
  filter(sale_date == 2016,
         assessment_year == 2016) %>% 
  mutate(overassessed = as_factor(if_else(sale_price < (assessed_value * 2),
                                1,
                                0)))

split <- rsample::initial_split(overassessed)

train <- training(split)
test <- testing(split)

#add in neighborhood!
log_model <- 
  logistic_reg() %>%
  set_engine('glm') %>%
  set_mode('classification')


# tree_model <-
#   parsnip::decision_tree(tree_depth=5) %>%
#   set_engine("rpart") %>%
#   set_mode('regression')

overassessed_recipe <- 
  recipe(overassessed ~ sale_price + ward + zip_code,
      data = train) %>%
  step_other(ward, threshold = 0.01) %>% 
  step_other(zip_code, threshold = 0.01) %>% 
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())
  
log_wflow <- 
  workflow() %>%
  add_model(log_model) %>% 
  add_recipe(overassessed_recipe)

#Evaluate this
log_fit <- fit(log_wflow, overassessed)


#Evaluate according to the charts

```


Need to do a yardstick evaluation at the bottom of this work:
```{r}

# yardstick::mape(divvy_preds, 
#      truth = rides,
#      estimate = .pred)
# yardstick::rmse(divvy_preds, 
#      truth = rides,
#      estimate = .pred)
```
